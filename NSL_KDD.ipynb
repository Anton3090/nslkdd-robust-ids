{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 394223,
          "sourceType": "datasetVersion",
          "datasetId": 174616
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NSL-KDD",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anton3090/nslkdd-robust-ids/blob/main/NSL_KDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "hassan06_nslkdd_path = kagglehub.dataset_download('hassan06/nslkdd')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "x0yrybq5Z7PA",
        "outputId": "3cbc302a-bd34-4235-fbdf-63575b13d742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:42:45.222615Z",
          "iopub.execute_input": "2025-05-20T18:42:45.222874Z",
          "iopub.status.idle": "2025-05-20T18:42:45.498833Z",
          "shell.execute_reply.started": "2025-05-20T18:42:45.222852Z",
          "shell.execute_reply": "2025-05-20T18:42:45.49807Z"
        },
        "id": "UsNj496sZ7PH",
        "outputId": "54cee2ad-3c43-4c07-f864-22f191fb073e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/nslkdd/KDDTest+.arff\n",
            "/kaggle/input/nslkdd/KDDTest-21.arff\n",
            "/kaggle/input/nslkdd/KDDTest1.jpg\n",
            "/kaggle/input/nslkdd/KDDTrain+.txt\n",
            "/kaggle/input/nslkdd/KDDTrain+_20Percent.txt\n",
            "/kaggle/input/nslkdd/KDDTest-21.txt\n",
            "/kaggle/input/nslkdd/KDDTest+.txt\n",
            "/kaggle/input/nslkdd/KDDTrain+.arff\n",
            "/kaggle/input/nslkdd/index.html\n",
            "/kaggle/input/nslkdd/KDDTrain+_20Percent.arff\n",
            "/kaggle/input/nslkdd/KDDTrain1.jpg\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTest+.arff\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.arff\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTest1.jpg\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.txt\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.txt\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTest-21.txt\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTest+.txt\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTrain+.arff\n",
            "/kaggle/input/nslkdd/nsl-kdd/index.html\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTrain+_20Percent.arff\n",
            "/kaggle/input/nslkdd/nsl-kdd/KDDTrain1.jpg\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Import**"
      ],
      "metadata": {
        "id": "V7qPEu8nZ7PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Download and load the dataset\n",
        "path = kagglehub.dataset_download(\"hassan06/nslkdd\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Load train and test datasets\n",
        "train_df = pd.read_csv(f\"{path}/KDDTrain+.txt\", header=None)\n",
        "test_df = pd.read_csv(f\"{path}/KDDTest+.txt\", header=None)\n",
        "\n",
        "# NSL-KDD has 41 features + 1 label column\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "    'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "    'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "    'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate', 'label', 'difficulty_level'  # added last column\n",
        "]\n",
        "\n",
        "train_df.columns = test_df.columns = columns\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:42:45.499856Z",
          "iopub.execute_input": "2025-05-20T18:42:45.500209Z",
          "iopub.status.idle": "2025-05-20T18:42:47.508998Z",
          "shell.execute_reply.started": "2025-05-20T18:42:45.50019Z",
          "shell.execute_reply": "2025-05-20T18:42:47.508036Z"
        },
        "id": "ZJVbmC5HZ7PO",
        "outputId": "e3af469b-ffdf-4463-e0d9-6fae0bac8540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/nslkdd\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "JpGjeHJjZ7PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary classification: normal vs attack\n",
        "train_df['label'] = train_df['label'].apply(lambda x: 'normal' if x == 'normal' else 'attack')\n",
        "test_df['label'] = test_df['label'].apply(lambda x: 'normal' if x == 'normal' else 'attack')\n",
        "\n",
        "# Encode categorical features\n",
        "cat_cols = ['protocol_type', 'service', 'flag']\n",
        "encoder = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    train_df[col] = encoder.fit_transform(train_df[col])\n",
        "    test_df[col] = encoder.transform(test_df[col])\n",
        "\n",
        "# Separate features and labels\n",
        "X_train = train_df.drop('label', axis=1)\n",
        "y_train = LabelEncoder().fit_transform(train_df['label'])\n",
        "\n",
        "X_test = test_df.drop('label', axis=1)\n",
        "y_test = LabelEncoder().fit_transform(test_df['label'])\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:42:47.509923Z",
          "iopub.execute_input": "2025-05-20T18:42:47.510304Z",
          "iopub.status.idle": "2025-05-20T18:42:47.827105Z",
          "shell.execute_reply.started": "2025-05-20T18:42:47.510274Z",
          "shell.execute_reply": "2025-05-20T18:42:47.826387Z"
        },
        "id": "zReKu6WKZ7PS"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build & Train Deep Learning Model**"
      ],
      "metadata": {
        "id": "gfX7IIabZ7PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=128, shuffle=True)\n",
        "\n",
        "# Define model\n",
        "class IDSModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(IDSModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = IDSModel(X_train.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10):\n",
        "    for xb, yb in train_loader:\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:42:47.828638Z",
          "iopub.execute_input": "2025-05-20T18:42:47.828852Z",
          "iopub.status.idle": "2025-05-20T18:43:22.932093Z",
          "shell.execute_reply.started": "2025-05-20T18:42:47.828835Z",
          "shell.execute_reply": "2025-05-20T18:43:22.931465Z"
        },
        "id": "yeHuWu66Z7PV",
        "outputId": "37787d55-eef3-4147-99b5-fbadd47dad73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.0100\n",
            "Epoch 2 - Loss: 0.0014\n",
            "Epoch 3 - Loss: 0.0079\n",
            "Epoch 4 - Loss: 0.0000\n",
            "Epoch 5 - Loss: 0.0253\n",
            "Epoch 6 - Loss: 0.0007\n",
            "Epoch 7 - Loss: 0.0000\n",
            "Epoch 8 - Loss: 0.0172\n",
            "Epoch 9 - Loss: 0.0006\n",
            "Epoch 10 - Loss: 0.0003\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "N9s6ueJYZ7PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = torch.argmax(model(X_test_tensor), dim=1)\n",
        "    acc = (preds == y_test_tensor).float().mean()\n",
        "print(\"Test Accuracy:\", acc.item())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:43:22.932826Z",
          "iopub.execute_input": "2025-05-20T18:43:22.933301Z",
          "iopub.status.idle": "2025-05-20T18:43:22.95123Z",
          "shell.execute_reply.started": "2025-05-20T18:43:22.933273Z",
          "shell.execute_reply": "2025-05-20T18:43:22.950605Z"
        },
        "id": "YR_xzWSUZ7PZ",
        "outputId": "c7aefb16-35b5-4330-8a21-ec5d94942cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8662615418434143\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-Time Packet Detection with Scapy**"
      ],
      "metadata": {
        "id": "MmpZtomfZ7Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), 'ids_model.pth')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:43:22.951865Z",
          "iopub.execute_input": "2025-05-20T18:43:22.952072Z",
          "iopub.status.idle": "2025-05-20T18:43:22.958734Z",
          "shell.execute_reply.started": "2025-05-20T18:43:22.952055Z",
          "shell.execute_reply": "2025-05-20T18:43:22.95814Z"
        },
        "id": "R3V8EQHzZ7Pc"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scapy torch numpy joblib\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:43:22.959512Z",
          "iopub.execute_input": "2025-05-20T18:43:22.959734Z",
          "iopub.status.idle": "2025-05-20T18:44:49.793566Z",
          "shell.execute_reply.started": "2025-05-20T18:43:22.959719Z",
          "shell.execute_reply": "2025-05-20T18:44:49.792739Z"
        },
        "id": "YTUZXnUlZ7Pe",
        "outputId": "7f622335-b60d-422c-a5b4-7e499ccffee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scapy\n",
            "  Downloading scapy-2.6.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading scapy-2.6.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scapy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scapy-2.6.1\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Example: Fit scaler to your training features\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  # X_train should be your training features (without labels)\n",
        "\n",
        "# Save to a file\n",
        "joblib.dump(scaler, \"scaler.save\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:44:49.794756Z",
          "iopub.execute_input": "2025-05-20T18:44:49.795126Z",
          "iopub.status.idle": "2025-05-20T18:44:49.872811Z",
          "shell.execute_reply.started": "2025-05-20T18:44:49.795093Z",
          "shell.execute_reply": "2025-05-20T18:44:49.872053Z"
        },
        "id": "Y4eEvZJdZ7Pf",
        "outputId": "3601548c-2bb2-4753-b693-4d58d33d43f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.save']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "from scapy.all import sniff, IP, TCP, UDP\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the same model architecture\n",
        "class IDSModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(IDSModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 2)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Load model and scaler\n",
        "model = IDSModel(input_dim=42)  # NSL-KDD has 41 features\n",
        "model.load_state_dict(torch.load(\"ids_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load(\"scaler.save\")\n",
        "\n",
        "# Example feature extractor (customize to match training features)\n",
        "def extract_features(pkt):\n",
        "    try:\n",
        "        # Very basic feature simulation\n",
        "        length = len(pkt)\n",
        "        ttl = pkt[IP].ttl if IP in pkt else 0\n",
        "        dport = pkt[TCP].dport if TCP in pkt else (pkt[UDP].dport if UDP in pkt else 0)\n",
        "        features = [length, ttl, dport]\n",
        "        features += [0] * (42 - len(features))  # Pad to 41 features\n",
        "        return np.array(features).reshape(1, -1)\n",
        "    except:\n",
        "        return np.zeros((1, 41))  # Return dummy data on failure\n",
        "\n",
        "# Packet classification and logging\n",
        "def classify_packet(pkt):\n",
        "    features = extract_features(pkt)\n",
        "    scaled = scaler.transform(features)\n",
        "    tensor = torch.tensor(scaled, dtype=torch.float32)\n",
        "    output = model(tensor)\n",
        "    pred = torch.argmax(output).item()\n",
        "    label = \"attack\" if pred == 1 else \"normal\"\n",
        "\n",
        "    # Print & log\n",
        "    print(f\"[{datetime.now()}] Packet classified as: {label}\")\n",
        "    with open(\"log.txt\", \"a\") as f:\n",
        "        f.write(f\"{datetime.now()} | {pkt.summary()} | Result: {label}\\n\")\n",
        "\n",
        "# Start sniffing\n",
        "print(\"Sniffing... Press Ctrl+C to stop.\")\n",
        "sniff(prn=classify_packet, count=200)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:44:49.873758Z",
          "iopub.execute_input": "2025-05-20T18:44:49.874058Z",
          "iopub.status.idle": "2025-05-20T18:45:06.860545Z",
          "shell.execute_reply.started": "2025-05-20T18:44:49.874033Z",
          "shell.execute_reply": "2025-05-20T18:45:06.859958Z"
        },
        "id": "LNyNNCLIZ7Pg",
        "outputId": "ebfea069-af24-48a1-8042-0dc19c69ab58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sniffing... Press Ctrl+C to stop.\n",
            "[2025-05-20 20:38:03.457974] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.460795] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.670040] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.672376] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.685652] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.687751] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.689522] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.700061] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.702143] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.704390] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.708900] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.710808] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.712649] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.714329] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.800954] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.804132] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.877905] Packet classified as: normal\n",
            "[2025-05-20 20:38:03.880044] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.088196] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.090801] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.292841] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.295016] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.499115] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.501749] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.704732] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.707185] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.909946] Packet classified as: normal\n",
            "[2025-05-20 20:38:04.912158] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.115145] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.117274] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.322614] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.325014] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.530228] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.532911] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.740928] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.747694] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.947774] Packet classified as: normal\n",
            "[2025-05-20 20:38:05.949945] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.153115] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.155282] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.358577] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.361194] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.566197] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.570119] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.774095] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.776294] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.980331] Packet classified as: normal\n",
            "[2025-05-20 20:38:06.982596] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.187760] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.189916] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.393965] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.399841] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.599407] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.602092] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.805474] Packet classified as: normal\n",
            "[2025-05-20 20:38:07.808233] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.014326] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.016781] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.220748] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.223072] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.428302] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.430464] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.635663] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.637834] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.846438] Packet classified as: normal\n",
            "[2025-05-20 20:38:08.850074] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.055201] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.057503] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.261641] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.263890] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.467571] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.470045] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.673764] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.676148] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.881593] Packet classified as: normal\n",
            "[2025-05-20 20:38:09.884008] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.088930] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.091424] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.296488] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.298940] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.504285] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.506669] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.711929] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.714544] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.919349] Packet classified as: normal\n",
            "[2025-05-20 20:38:10.921477] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.127293] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.129764] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.335018] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.337793] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.543902] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.546877] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.751294] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.753874] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.958156] Packet classified as: normal\n",
            "[2025-05-20 20:38:11.965106] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.170585] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.179167] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.383800] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.386017] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.527205] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.529938] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.531894] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.533796] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.535403] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.537080] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.539385] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.540896] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.542376] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.543841] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.593141] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.595410] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.801779] Packet classified as: normal\n",
            "[2025-05-20 20:38:12.804010] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.008188] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.010367] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.220181] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.222222] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.426956] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.438650] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.647221] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.649652] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.857917] Packet classified as: normal\n",
            "[2025-05-20 20:38:13.860121] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.086447] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.088511] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.292786] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.294908] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.383524] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.385472] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.390564] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.392488] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.394443] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.401348] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.409345] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.411001] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.412630] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.414463] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.437245] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.446619] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.448302] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.449812] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.451169] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.501908] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.506096] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.711173] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.725291] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.925700] Packet classified as: normal\n",
            "[2025-05-20 20:38:14.927983] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.131690] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.133957] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.341205] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.343040] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.550959] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.553384] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.757514] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.759935] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.962829] Packet classified as: normal\n",
            "[2025-05-20 20:38:15.969755] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.174503] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.176466] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.391236] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.393498] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.597961] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.600788] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.806773] Packet classified as: normal\n",
            "[2025-05-20 20:38:16.811296] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.022125] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.024622] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.230056] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.232416] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.435852] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.440691] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.553416] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.556701] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.640333] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.642143] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.844950] Packet classified as: normal\n",
            "[2025-05-20 20:38:17.847001] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.049824] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.051657] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.254715] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.256600] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.459424] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.461223] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.663986] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.665787] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.868453] Packet classified as: normal\n",
            "[2025-05-20 20:38:18.870205] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.073439] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.075616] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.279218] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.281396] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.483798] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.485620] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.581783] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.584027] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.586473] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.588606] Packet classified as: normal\n",
            "[2025-05-20 20:38:19.689437] Packet classified as: normal\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sniffed: TCP:200 UDP:0 ICMP:0 Other:0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logging to a Text File**"
      ],
      "metadata": {
        "id": "OQBI73INZ7Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def log_packet(pkt, result):\n",
        "    with open(\"log.txt\", \"a\") as f:\n",
        "        f.write(f\"{datetime.datetime.now()} | {pkt.summary()} | Result: {result}\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:45:06.862114Z",
          "iopub.execute_input": "2025-05-20T18:45:06.863083Z",
          "iopub.status.idle": "2025-05-20T18:45:06.866671Z",
          "shell.execute_reply.started": "2025-05-20T18:45:06.863056Z",
          "shell.execute_reply": "2025-05-20T18:45:06.865962Z"
        },
        "id": "Q0AAPH4FZ7Pk"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Secure Model with Adversarial Robustness Toolbox (ART)**"
      ],
      "metadata": {
        "id": "jNy95TswZ7Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adversarial-robustness-toolbox\n",
        "\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Convert X_test to float32 numpy array before generating adversarial examples\n",
        "X_test_float32 = X_test.astype(np.float32)\n",
        "\n",
        "classifier = PyTorchClassifier(\n",
        "    model=model,\n",
        "    loss=criterion,\n",
        "    optimizer=optimizer,\n",
        "    input_shape=(X_train.shape[1],),\n",
        "    nb_classes=2,\n",
        ")\n",
        "\n",
        "# Generate adversarial examples using float32 inputs\n",
        "fgsm = FastGradientMethod(estimator=classifier, eps=0.1)\n",
        "X_test_adv = fgsm.generate(X_test_float32)\n",
        "\n",
        "# Predict on adversarial examples\n",
        "preds = np.argmax(classifier.predict(X_test_adv), axis=1)\n",
        "\n",
        "accuracy = np.mean(preds == y_test)\n",
        "print(\"Robust Accuracy under FGSM attack:\", accuracy)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-20T18:45:06.867513Z",
          "iopub.execute_input": "2025-05-20T18:45:06.8683Z",
          "iopub.status.idle": "2025-05-20T18:45:17.023913Z",
          "shell.execute_reply.started": "2025-05-20T18:45:06.868277Z",
          "shell.execute_reply": "2025-05-20T18:45:17.023257Z"
        },
        "id": "xPHPg1O3Z7Pn",
        "outputId": "988c337b-8b59-4fc7-f447-2e3bf935cce1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Downloading adversarial_robustness_toolbox-1.19.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.19.1\n",
            "Robust Accuracy under FGSM attack: 0.8226135557132718\n"
          ]
        }
      ],
      "execution_count": 12
    }
  ]
}